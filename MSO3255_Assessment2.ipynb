{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaVrabcova/Assessment_2_Mini_Project/blob/main/MSO3255_Assessment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53NDKH8SpLMw"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6J6Zf6pUr2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import numpy as np\n",
        "import string, re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gEKiwLOfwED"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvqkWkzAXAP8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.utils.prune as prune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BkjMgpP6rTw"
      },
      "source": [
        "# ========================\n",
        "# FINAL CONFIGURATION:\n",
        "# Includes pruning, intermediate distillation, best Î±=0.5 and T=4.0\n",
        "# ========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NJm4mE4eVZI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHVRE9aLKDfN"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# CONFIG\n",
        "###############################################################################\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2  # Increase for better results, # Note: One additional epoch is run immediately after pruning (total = 3 epochs)\n",
        "LEARNING_RATE = 3e-5\n",
        "\n",
        "# Distillation hyperparams\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 4.0  # Temperature for soft logits\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM2lnlMVpfWn"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# 1) DATA LOADING & PREPROCESSING\n",
        "###############################################################################\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "\n",
        "# For demonstration, we'll use the full training set or a subset\n",
        "train_data = raw_squad[\"train\"].select(range(5000))\n",
        "val_data = raw_squad[\"validation\"].select(range(1000))  # Subset of 1k for speed\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess_function(ex):\n",
        "    \"\"\"\n",
        "    Tokenize question + context and try to map answer start/end to token indices.\n",
        "    We'll do a naive single-chunk approach (no sliding window).\n",
        "    \"\"\"\n",
        "    # SQuAD \"answers\" has a list of possible answers; we take the first\n",
        "    start_char = ex[\"answers\"][\"answer_start\"][0]\n",
        "    ans_texts = ex[\"answers\"][\"text\"]\n",
        "    answer_text = ans_texts[0] if len(ans_texts) > 0 else \"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        ex[\"question\"],\n",
        "        ex[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True  # We'll use offsets for naive char->token mapping\n",
        "    )\n",
        "\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "\n",
        "    # Find start/end token indices\n",
        "    start_token_idx = 0\n",
        "    end_token_idx = 0\n",
        "\n",
        "    # End char\n",
        "    end_char = start_char + len(answer_text)\n",
        "\n",
        "    # loop through offsets to find the best match\n",
        "    for i, (off_start, off_end) in enumerate(offsets):\n",
        "        # Some offsets may be None or special tokens\n",
        "        if off_start is None or off_end is None:\n",
        "            continue\n",
        "        if off_start <= start_char < off_end:\n",
        "            start_token_idx = i\n",
        "        if off_start < end_char <= off_end:\n",
        "            end_token_idx = i\n",
        "            break\n",
        "\n",
        "    if end_token_idx < start_token_idx:\n",
        "        end_token_idx = start_token_idx\n",
        "\n",
        "    # Store in encoding\n",
        "    encoding[\"start_positions\"] = start_token_idx\n",
        "    encoding[\"end_positions\"] = end_token_idx\n",
        "\n",
        "    # Remove offset mapping to reduce data size\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    return encoding\n",
        "\n",
        "train_processed = train_data.map(preprocess_function)\n",
        "val_processed   = val_data.map(preprocess_function)\n",
        "\n",
        "# We'll convert to PyTorch Tensors\n",
        "def to_tensor_dataset(hf_dataset):\n",
        "    input_ids = torch.tensor(hf_dataset[\"input_ids\"], dtype=torch.long)\n",
        "    attention_mask = torch.tensor(hf_dataset[\"attention_mask\"], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor(hf_dataset[\"token_type_ids\"], dtype=torch.long) \\\n",
        "        if \"token_type_ids\" in hf_dataset.features else None\n",
        "    start_positions = torch.tensor(hf_dataset[\"start_positions\"], dtype=torch.long)\n",
        "    end_positions   = torch.tensor(hf_dataset[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        return TensorDataset(input_ids, attention_mask, token_type_ids, start_positions, end_positions)\n",
        "    else:\n",
        "        # For models without token_type_ids (like DistilBERT)\n",
        "        return TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "\n",
        "train_tds = to_tensor_dataset(train_processed)\n",
        "val_tds   = to_tensor_dataset(val_processed)\n",
        "\n",
        "train_loader = DataLoader(train_tds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader   = DataLoader(val_tds,   batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4NcBFhsTp541"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# 2) TEACHER MODEL INFERENCE (collect teacher logits)\n",
        "###############################################################################\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "teacher_start_logits_list = []\n",
        "teacher_end_logits_list   = []\n",
        "gt_start_list = []\n",
        "gt_end_list   = []\n",
        "input_tensors = []\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        count+=1\n",
        "        print(\"batch\", count)\n",
        "        # batch can have 4 or 5 tensors depending on token_type_ids existence\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "        start_pos = start_pos.to(DEVICE)\n",
        "        end_pos   = end_pos.to(DEVICE)\n",
        "\n",
        "        outputs = teacher_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Collect logits\n",
        "        teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "        teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "\n",
        "        gt_start_list.append(start_pos.cpu())\n",
        "        gt_end_list.append(end_pos.cpu())\n",
        "\n",
        "        # We'll store the CPU tensors of inputs for the student dataset\n",
        "        if token_type_ids is not None:\n",
        "            input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu()))\n",
        "        else:\n",
        "            input_tensors.append((input_ids.cpu(), attention_mask.cpu(), None))\n",
        "\n",
        "# Concatenate teacher outputs\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full   = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full   = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten input tensors\n",
        "all_input_ids         = []\n",
        "all_attention_masks   = []\n",
        "all_token_type_ids    = []\n",
        "for batch_data in input_tensors:\n",
        "    i_ids, i_mask, i_type = batch_data\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "    else:\n",
        "        all_token_type_ids = None\n",
        "\n",
        "all_input_ids       = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "\n",
        "# Save teacher logits to Google Drive\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/teacher_logits.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"start_logits\": teacher_start_logits_full,\n",
        "    \"end_logits\": teacher_end_logits_full,\n",
        "    \"gt_start\": gt_start_full,\n",
        "    \"gt_end\": gt_end_full,\n",
        "    \"input_ids\": all_input_ids,\n",
        "    \"attention_mask\": all_attention_masks,\n",
        "    \"token_type_ids\": all_token_type_ids,\n",
        "}, LOGITS_PATH)\n",
        "\n",
        "print(f\" Teacher logits saved successfully at: {LOGITS_PATH}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H1OCZBXxhXm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Set path to the saved logits file in Google Drive\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/teacher_logits.pt\"\n",
        "\n",
        "# Check and load\n",
        "if os.path.exists(LOGITS_PATH):\n",
        "    print(f\" Loading teacher logits from: {LOGITS_PATH}\")\n",
        "    data = torch.load(LOGITS_PATH)\n",
        "\n",
        "    teacher_start_logits_full = data[\"start_logits\"]\n",
        "    teacher_end_logits_full   = data[\"end_logits\"]\n",
        "    gt_start_full             = data[\"gt_start\"]\n",
        "    gt_end_full               = data[\"gt_end\"]\n",
        "    all_input_ids             = data[\"input_ids\"]\n",
        "    all_attention_masks       = data[\"attention_mask\"]\n",
        "    all_token_type_ids        = data[\"token_type_ids\"]\n",
        "\n",
        "    print(\"Teacher logits loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Teacher logits file not found at {LOGITS_PATH}. Please run inference to generate and save the logits first.\")\n",
        "\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL8DLmvCC2hr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert teacher logits into probabilities using softmax with temperature\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs   = softmax_with_temperature(teacher_end_logits_full, TEMPERATURE)\n",
        "\n",
        "# Construct distillation dataset\n",
        "if all_token_type_ids is not None:\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_masks,\n",
        "        all_token_type_ids,\n",
        "        teacher_start_probs,\n",
        "        teacher_end_probs,\n",
        "        gt_start_full,\n",
        "        gt_end_full\n",
        "    )\n",
        "else:\n",
        "    # For models without token_type_ids\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_masks,\n",
        "        torch.zeros_like(all_input_ids),  # dummy placeholder\n",
        "        teacher_start_probs,\n",
        "        teacher_end_probs,\n",
        "        gt_start_full,\n",
        "        gt_end_full\n",
        "    )\n",
        "\n",
        "# Define DataLoader\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxnZjsDfFrRS"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# 3) DEFINE A SMALLER STUDENT MODEL\n",
        "###############################################################################\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,         # smaller hidden dim\n",
        "    num_hidden_layers=8,     # fewer layers\n",
        "    num_attention_heads=8,   # fewer heads\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeK8aRJGB46z"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# 4) Distillation + Pruning\n",
        "###############################################################################\n",
        "\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "\n",
        "# Constants needed for distillation\n",
        "TEACHER_LAYER_TO_MATCH = 8\n",
        "STUDENT_LAYER_TO_MATCH = 4\n",
        "HIDDEN_LOSS_WEIGHT = 0.3\n",
        "\n",
        "# Loss and projection layer\n",
        "mse_loss = nn.MSELoss()\n",
        "project_teacher = nn.Linear(1024, 384).to(DEVICE)\n",
        "\n",
        "# Define helper functions\n",
        "def apply_pruning(model, amount=0.1):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            prune.remove(module, \"weight\")\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "# Apply pruning\n",
        "apply_pruning(student_model, amount=0.1)\n",
        "print(\"Applied pruning. Remaining trainable parameters:\", count_parameters(student_model))\n",
        "\n",
        "# Re-initialize optimizer after pruning\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Distillation training step\n",
        "def distillation_train_step(batch_data):\n",
        "    if all_token_type_ids is not None:\n",
        "        input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "    else:\n",
        "        input_ids, attention_mask, _, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = None\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "    t_start_probs = t_start_probs.to(DEVICE)\n",
        "    t_end_probs   = t_end_probs.to(DEVICE)\n",
        "    gt_start = gt_start.to(DEVICE)\n",
        "    gt_end   = gt_end.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        teacher_outputs = teacher_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        teacher_hidden = teacher_outputs.hidden_states[TEACHER_LAYER_TO_MATCH]\n",
        "        teacher_hidden_proj = project_teacher(teacher_hidden)\n",
        "\n",
        "    student_outputs = student_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    student_start_logits = student_outputs.start_logits\n",
        "    student_end_logits   = student_outputs.end_logits\n",
        "    student_hidden = student_outputs.hidden_states[STUDENT_LAYER_TO_MATCH]\n",
        "\n",
        "    # Hard loss (cross-entropy with labels)\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_start_hard = ce_loss_fn(student_start_logits, gt_start)\n",
        "    loss_end_hard   = ce_loss_fn(student_end_logits,   gt_end)\n",
        "    hard_loss = 0.5 * (loss_start_hard + loss_end_hard)\n",
        "\n",
        "    # Soft loss (KL with teacher logits)\n",
        "    s_start_probs = softmax_with_temperature(student_start_logits, TEMPERATURE)\n",
        "    s_end_probs   = softmax_with_temperature(student_end_logits,   TEMPERATURE)\n",
        "\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    start_kl = kl_loss_fn(s_start_probs.log(), t_start_probs)\n",
        "    end_kl   = kl_loss_fn(s_end_probs.log(),   t_end_probs)\n",
        "    soft_loss = 0.5 * (start_kl + end_kl)\n",
        "\n",
        "    # Hidden state loss (internal distillation)\n",
        "    hidden_loss = mse_loss(student_hidden, teacher_hidden_proj)\n",
        "\n",
        "    # Final loss\n",
        "    loss = ALPHA * soft_loss + (1 - ALPHA) * hard_loss + HIDDEN_LOSS_WEIGHT * hidden_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), hard_loss.item(), soft_loss.item(), hidden_loss.item()\n",
        "\n",
        "# Run one epoch of fine-tuning after pruning\n",
        "print(\"\\n Fine-tuning pruned model for 1 additional epoch...\")\n",
        "student_model.train()\n",
        "epoch_losses = []\n",
        "\n",
        "for step, batch_data in enumerate(distill_loader):\n",
        "    loss_val, hard_val, soft_val, hidden_val = distillation_train_step(batch_data)\n",
        "    epoch_losses.append(loss_val)\n",
        "    if step % 200 == 0:\n",
        "        print(f\"  Step {step}: Total Loss = {loss_val:.4f} (Hard: {hard_val:.4f}, Soft: {soft_val:.4f}, Hidden: {hidden_val:.4f})\")\n",
        "\n",
        "print(f\" Fine-tuning Loss (Epoch Avg): {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rESxOq0mm9g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epoch_losses)\n",
        "plt.title(\"Training Loss During Fine-Tuning (Post-Pruning)\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGhPJsw4pFJZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5) POST-PROCESSING & EVALUATION\n",
        "###############################################################################\n",
        "# We'll do a naive approach: take argmax of start/end, decode, compare with ground truth.\n",
        "val_contexts = val_data[\"context\"]\n",
        "val_questions = val_data[\"question\"]\n",
        "val_answers = val_data[\"answers\"]  # list of dicts with \"text\", \"answer_start\"\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return \"\".join(ch for ch in txt if ch not in string.punctuation)\n",
        "\n",
        "    s = s.lower()\n",
        "    s = remove_articles(s)\n",
        "    s = remove_punc(s)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "student_model.eval()\n",
        "em_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "val_dataloader = DataLoader(val_tds, batch_size=8, shuffle=False)\n",
        "offset = 0  # to track the global index in val_data\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        outputs = student_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits   = outputs.end_logits\n",
        "\n",
        "        start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "        end_indices   = torch.argmax(end_logits,   dim=1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(start_indices)):\n",
        "            global_idx = offset + i\n",
        "            if global_idx >= len(val_contexts):\n",
        "                continue\n",
        "\n",
        "            s_ind = start_indices[i]\n",
        "            e_ind = end_indices[i]\n",
        "            if e_ind < s_ind:\n",
        "                e_ind = s_ind\n",
        "\n",
        "            # Decode predicted tokens\n",
        "            tokens_ = input_ids[i][s_ind : e_ind+1].cpu().numpy().tolist()\n",
        "            pred_text = tokenizer.decode(tokens_, skip_special_tokens=True)\n",
        "\n",
        "            # Ground truth: we pick the first answer\n",
        "            gold_answers = val_answers[global_idx][\"text\"]\n",
        "            if len(gold_answers) > 0:\n",
        "                gold_answer = gold_answers[0]\n",
        "            else:\n",
        "                gold_answer = \"\"\n",
        "\n",
        "            em = compute_exact_match(pred_text, gold_answer)\n",
        "            f1 = compute_f1(pred_text, gold_answer)\n",
        "\n",
        "            em_scores.append(em)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "        offset += len(start_indices)\n",
        "\n",
        "avg_em = np.mean(em_scores) * 100\n",
        "avg_f1 = np.mean(f1_scores) * 100\n",
        "print(f\"  Exact Match: {avg_em:.2f}%\")\n",
        "print(f\"  F1 Score:    {avg_f1:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNyatBRJ68v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def ask_question(question: str, context: str, model, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Given a question and a context, use the provided model to\n",
        "    predict the answer span and return the decoded string answer.\n",
        "    \"\"\"\n",
        "    # 1) Encode inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    token_type_ids = None\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # 2) Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # 3) Get predicted start/end token indices\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Ensure the end_index is >= start_index\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    # 4) Decode tokens back to string\n",
        "    answer_ids = input_ids[0, start_index : end_index+1]\n",
        "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    return answer_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6l798XWv8mx"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# Final demonstration: Validate student model works\n",
        "# -------------------------------------------------\n",
        "\n",
        "question = \"Which city is Galatasaray based in?\"\n",
        "context = \"Galatasaray, is a Turkish professional football club based on the European side of the city of Istanbul. It is founded in 1905. The team traditionally play in dark shades of red and yellow at home.\"\n",
        "\n",
        "# Evaluate with teacher model\n",
        "teacher_model.eval()\n",
        "teacher_answer = ask_question(question, context, teacher_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Teacher Answer]: {teacher_answer}\")\n",
        "\n",
        "# Evaluate with student model\n",
        "student_model.eval()\n",
        "student_answer = ask_question(question, context, student_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Student Answer]: {student_answer}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvU-cFZuv6-R"
      },
      "outputs": [],
      "source": [
        "teacher_model.eval()\n",
        "student_model.eval()\n",
        "\n",
        "sample_question = \"Who wrote the novel 1984?\"\n",
        "sample_context = (\n",
        "    \"George Orwell was a British author best known for his novels Animal Farm and 1984. \"\n",
        "    \"He was a critic of totalitarianism and wrote extensively about political issues.\"\n",
        ")\n",
        "\n",
        "student_answer = ask_question(sample_question, sample_context, student_model, tokenizer, device=DEVICE)\n",
        "teacher_answer = ask_question(sample_question, sample_context, teacher_model, tokenizer, device=DEVICE)\n",
        "\n",
        "print(f\"Question: {sample_question}\")\n",
        "print(f\"Student Answer: {student_answer}\")\n",
        "print(f\"Teacher Answer: {teacher_answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6FXc20Qn9G3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# EM and F1 values from your experiments\n",
        "labels = [\"Baseline\", \"Distilled+Tuned\", \"Pruned+Fine-tuned\"]\n",
        "em_scores = [2.8, 2.0, 1.3]\n",
        "f1_scores = [6.9, 6.59, 5.3]\n",
        "\n",
        "x = np.arange(len(labels))  # label locations\n",
        "width = 0.35  # bar width\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, em_scores, width, label='Exact Match (EM)', color='skyblue')\n",
        "rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='lightgreen')\n",
        "\n",
        "# Add text labels\n",
        "ax.set_ylabel('Score (%)')\n",
        "ax.set_title('EM and F1 Score Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylim(0, 8)\n",
        "ax.legend()\n",
        "\n",
        "def add_labels(rects):\n",
        "    for r in rects:\n",
        "        height = r.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(r.get_x() + r.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_labels(rects1)\n",
        "add_labels(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}