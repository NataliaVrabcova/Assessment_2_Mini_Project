{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaVrabcova/Assessment_2_Mini_Project/blob/main/MSO3255_Assessment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import numpy as np\n",
        "import string, re\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.utils.prune as prune"
      ],
      "metadata": {
        "id": "uRkYTIkv13lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# CONFIG\n",
        "###############################################################################\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2  # Increase for better results, # Note: One additional epoch is run immediately after pruning (total = 3 epochs)\n",
        "LEARNING_RATE = 3e-5\n",
        "\n",
        "# Distillation hyperparams\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 4.0  # Temperature for soft logits\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n"
      ],
      "metadata": {
        "id": "frA36Z0Z2RFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 1) DATA LOADING & PREPROCESSING\n",
        "###############################################################################\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "\n",
        "# For demonstration, we'll use the full training set or a subset\n",
        "train_data = raw_squad[\"train\"].select(range(5000))\n",
        "val_data = raw_squad[\"validation\"].select(range(1000))  # Subset of 1k for speed\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess_function(ex):\n",
        "    \"\"\"\n",
        "    Tokenize question + context and try to map answer start/end to token indices.\n",
        "    We'll do a naive single-chunk approach (no sliding window).\n",
        "    \"\"\"\n",
        "    # SQuAD \"answers\" has a list of possible answers; we take the first\n",
        "    start_char = ex[\"answers\"][\"answer_start\"][0]\n",
        "    ans_texts = ex[\"answers\"][\"text\"]\n",
        "    answer_text = ans_texts[0] if len(ans_texts) > 0 else \"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        ex[\"question\"],\n",
        "        ex[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True  # We'll use offsets for naive char->token mapping\n",
        "    )\n",
        "\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "\n",
        "    # Find start/end token indices\n",
        "    start_token_idx = 0\n",
        "    end_token_idx = 0\n",
        "\n",
        "    # End char\n",
        "    end_char = start_char + len(answer_text)\n",
        "\n",
        "    # loop through offsets to find the best match\n",
        "    for i, (off_start, off_end) in enumerate(offsets):\n",
        "        # Some offsets may be None or special tokens\n",
        "        if off_start is None or off_end is None:\n",
        "            continue\n",
        "        if off_start <= start_char < off_end:\n",
        "            start_token_idx = i\n",
        "        if off_start < end_char <= off_end:\n",
        "            end_token_idx = i\n",
        "            break\n",
        "\n",
        "    if end_token_idx < start_token_idx:\n",
        "        end_token_idx = start_token_idx\n",
        "\n",
        "    # Store in encoding\n",
        "    encoding[\"start_positions\"] = start_token_idx\n",
        "    encoding[\"end_positions\"] = end_token_idx\n",
        "\n",
        "    # Remove offset mapping to reduce data size\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    return encoding\n",
        "\n",
        "train_processed = train_data.map(preprocess_function)\n",
        "val_processed   = val_data.map(preprocess_function)\n",
        "\n",
        "# We'll convert to PyTorch Tensors\n",
        "def to_tensor_dataset(hf_dataset):\n",
        "    input_ids = torch.tensor(hf_dataset[\"input_ids\"], dtype=torch.long)\n",
        "    attention_mask = torch.tensor(hf_dataset[\"attention_mask\"], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor(hf_dataset[\"token_type_ids\"], dtype=torch.long) \\\n",
        "        if \"token_type_ids\" in hf_dataset.features else None\n",
        "    start_positions = torch.tensor(hf_dataset[\"start_positions\"], dtype=torch.long)\n",
        "    end_positions   = torch.tensor(hf_dataset[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        return TensorDataset(input_ids, attention_mask, token_type_ids, start_positions, end_positions)\n",
        "    else:\n",
        "        # For models without token_type_ids (like DistilBERT)\n",
        "        return TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "\n",
        "train_tds = to_tensor_dataset(train_processed)\n",
        "val_tds   = to_tensor_dataset(val_processed)\n",
        "\n",
        "train_loader = DataLoader(train_tds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader   = DataLoader(val_tds,   batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "NxdJs_Ji2UMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# 2) TEACHER MODEL INFERENCE (collect teacher logits)\n",
        "###############################################################################\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "teacher_start_logits_list = []\n",
        "teacher_end_logits_list   = []\n",
        "gt_start_list = []\n",
        "gt_end_list   = []\n",
        "input_tensors = []\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        count+=1\n",
        "        print(\"batch\", count)\n",
        "        # batch can have 4 or 5 tensors depending on token_type_ids existence\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "        start_pos = start_pos.to(DEVICE)\n",
        "        end_pos   = end_pos.to(DEVICE)\n",
        "\n",
        "        outputs = teacher_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Collect logits\n",
        "        teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "        teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "\n",
        "        gt_start_list.append(start_pos.cpu())\n",
        "        gt_end_list.append(end_pos.cpu())\n",
        "\n",
        "        # We'll store the CPU tensors of inputs for the student dataset\n",
        "        if token_type_ids is not None:\n",
        "            input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu()))\n",
        "        else:\n",
        "            input_tensors.append((input_ids.cpu(), attention_mask.cpu(), None))\n",
        "\n",
        "# Concatenate teacher outputs\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full   = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full   = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten input tensors\n",
        "all_input_ids         = []\n",
        "all_attention_masks   = []\n",
        "all_token_type_ids    = []\n",
        "for batch_data in input_tensors:\n",
        "    i_ids, i_mask, i_type = batch_data\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "    else:\n",
        "        all_token_type_ids = None\n",
        "\n",
        "all_input_ids       = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "\n",
        "# Save teacher logits to Google Drive\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/teacher_logits.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"start_logits\": teacher_start_logits_full,\n",
        "    \"end_logits\": teacher_end_logits_full,\n",
        "    \"gt_start\": gt_start_full,\n",
        "    \"gt_end\": gt_end_full,\n",
        "    \"input_ids\": all_input_ids,\n",
        "    \"attention_mask\": all_attention_masks,\n",
        "    \"token_type_ids\": all_token_type_ids,\n",
        "}, LOGITS_PATH)\n",
        "\n",
        "print(f\" Teacher logits saved successfully at: {LOGITS_PATH}\")"
      ],
      "metadata": {
        "id": "p4lKriSW2Yp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path to the saved logits file in Google Drive\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/teacher_logits.pt\"\n",
        "\n",
        "# Check and load\n",
        "if os.path.exists(LOGITS_PATH):\n",
        "    print(f\" Loading teacher logits from: {LOGITS_PATH}\")\n",
        "    data = torch.load(LOGITS_PATH)\n",
        "\n",
        "    teacher_start_logits_full = data[\"start_logits\"]\n",
        "    teacher_end_logits_full   = data[\"end_logits\"]\n",
        "    gt_start_full             = data[\"gt_start\"]\n",
        "    gt_end_full               = data[\"gt_end\"]\n",
        "    all_input_ids             = data[\"input_ids\"]\n",
        "    all_attention_masks       = data[\"attention_mask\"]\n",
        "    all_token_type_ids        = data[\"token_type_ids\"]\n",
        "\n",
        "    print(\"Teacher logits loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Teacher logits file not found at {LOGITS_PATH}. Please run inference to generate and save the logits first.\")\n",
        "\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()"
      ],
      "metadata": {
        "id": "WgZ6wqrT2c-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert teacher logits into probabilities using softmax with temperature\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs   = softmax_with_temperature(teacher_end_logits_full, TEMPERATURE)\n",
        "\n",
        "# Construct distillation dataset\n",
        "if all_token_type_ids is not None:\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_masks,\n",
        "        all_token_type_ids,\n",
        "        teacher_start_probs,\n",
        "        teacher_end_probs,\n",
        "        gt_start_full,\n",
        "        gt_end_full\n",
        "    )\n",
        "else:\n",
        "    # For models without token_type_ids\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_masks,\n",
        "        torch.zeros_like(all_input_ids),  # dummy placeholder\n",
        "        teacher_start_probs,\n",
        "        teacher_end_probs,\n",
        "        gt_start_full,\n",
        "        gt_end_full\n",
        "    )\n",
        "\n",
        "# Define DataLoader\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "kLjIU9xr2iSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 3) DEFINE A SMALLER STUDENT MODEL\n",
        "###############################################################################\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,         # smaller hidden dim\n",
        "    num_hidden_layers=8,     # fewer layers\n",
        "    num_attention_heads=8,   # fewer heads\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "ce-0ssBQ2iri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 4) Distillation + Pruning\n",
        "###############################################################################\n",
        "\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "\n",
        "# Constants needed for distillation\n",
        "TEACHER_LAYER_TO_MATCH = 8\n",
        "STUDENT_LAYER_TO_MATCH = 4\n",
        "HIDDEN_LOSS_WEIGHT = 0.3\n",
        "\n",
        "# Loss and projection layer\n",
        "mse_loss = nn.MSELoss()\n",
        "project_teacher = nn.Linear(1024, 384).to(DEVICE)\n",
        "\n",
        "# Define helper functions\n",
        "def apply_pruning(model, amount=0.1):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            prune.remove(module, \"weight\")\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "# Apply pruning\n",
        "apply_pruning(student_model, amount=0.1)\n",
        "print(\"Applied pruning. Remaining trainable parameters:\", count_parameters(student_model))\n",
        "\n",
        "# Re-initialize optimizer after pruning\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Distillation training step\n",
        "def distillation_train_step(batch_data):\n",
        "    if all_token_type_ids is not None:\n",
        "        input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "    else:\n",
        "        input_ids, attention_mask, _, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = None\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "    t_start_probs = t_start_probs.to(DEVICE)\n",
        "    t_end_probs   = t_end_probs.to(DEVICE)\n",
        "    gt_start = gt_start.to(DEVICE)\n",
        "    gt_end   = gt_end.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        teacher_outputs = teacher_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        teacher_hidden = teacher_outputs.hidden_states[TEACHER_LAYER_TO_MATCH]\n",
        "        teacher_hidden_proj = project_teacher(teacher_hidden)\n",
        "\n",
        "    student_outputs = student_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    student_start_logits = student_outputs.start_logits\n",
        "    student_end_logits   = student_outputs.end_logits\n",
        "    student_hidden = student_outputs.hidden_states[STUDENT_LAYER_TO_MATCH]\n",
        "\n",
        "    # Hard loss (cross-entropy with labels)\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_start_hard = ce_loss_fn(student_start_logits, gt_start)\n",
        "    loss_end_hard   = ce_loss_fn(student_end_logits,   gt_end)\n",
        "    hard_loss = 0.5 * (loss_start_hard + loss_end_hard)\n",
        "\n",
        "    # Soft loss (KL with teacher logits)\n",
        "    s_start_probs = softmax_with_temperature(student_start_logits, TEMPERATURE)\n",
        "    s_end_probs   = softmax_with_temperature(student_end_logits,   TEMPERATURE)\n",
        "\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    start_kl = kl_loss_fn(s_start_probs.log(), t_start_probs)\n",
        "    end_kl   = kl_loss_fn(s_end_probs.log(),   t_end_probs)\n",
        "    soft_loss = 0.5 * (start_kl + end_kl)\n",
        "\n",
        "    # Hidden state loss (internal distillation)\n",
        "    hidden_loss = mse_loss(student_hidden, teacher_hidden_proj)\n",
        "\n",
        "    # Final loss\n",
        "    loss = ALPHA * soft_loss + (1 - ALPHA) * hard_loss + HIDDEN_LOSS_WEIGHT * hidden_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), hard_loss.item(), soft_loss.item(), hidden_loss.item()\n",
        "\n",
        "# Run one epoch of fine-tuning after pruning\n",
        "print(\"\\n Fine-tuning pruned model for 1 additional epoch...\")\n",
        "student_model.train()\n",
        "epoch_losses = []\n",
        "\n",
        "for step, batch_data in enumerate(distill_loader):\n",
        "    loss_val, hard_val, soft_val, hidden_val = distillation_train_step(batch_data)\n",
        "    epoch_losses.append(loss_val)\n",
        "    if step % 200 == 0:\n",
        "        print(f\"  Step {step}: Total Loss = {loss_val:.4f} (Hard: {hard_val:.4f}, Soft: {soft_val:.4f}, Hidden: {hidden_val:.4f})\")\n",
        "\n",
        "print(f\" Fine-tuning Loss (Epoch Avg): {np.mean(epoch_losses):.4f}\")"
      ],
      "metadata": {
        "id": "1vQTCYmQ2nKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 5) OFFICIAL POST-PROCESSING & EVALUATION (SQuAD Evaluation)\n",
        "###############################################################################\n",
        "\n",
        "# Download the script and the dev set if not already present\n",
        "!wget -nc https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/evaluate-v1.1.py\n",
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
        "\n",
        "import json\n",
        "\n",
        "# Generate predictions dictionary: {qa_id: predicted_answer}\n",
        "predictions = {}\n",
        "offset = 0\n",
        "\n",
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "        end_indices = torch.argmax(end_logits, dim=1).cpu().numpy()\n",
        "\n",
        "        for j in range(len(start_indices)):\n",
        "            idx = offset + j\n",
        "            if idx >= len(val_data):\n",
        "                continue\n",
        "\n",
        "            s = start_indices[j]\n",
        "            e = end_indices[j]\n",
        "            if e < s:\n",
        "                e = s\n",
        "\n",
        "            tokens = input_ids[j][s:e+1].cpu().numpy().tolist()\n",
        "            pred_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "            qa_id = val_data[idx][\"id\"]\n",
        "            predictions[qa_id] = pred_text\n",
        "\n",
        "        offset += len(start_indices)\n",
        "\n",
        "# Save predictions to JSON\n",
        "with open(\"student_predictions.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)\n",
        "\n",
        "# Run the official SQuAD evaluation script\n",
        "!python evaluate-v1.1.py dev-v1.1.json student_predictions.json\n"
      ],
      "metadata": {
        "id": "mqwqclXPj-Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# 6. FINAL DEMONSTRATION: Validate Student Model\n",
        "# -----------------------------------------------\n",
        "def ask_question(question: str, context: str, model, tokenizer, device=DEVICE):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    token_type_ids = inputs.get(\"token_type_ids\")\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    answer_ids = input_ids[0, start_index : end_index+1]\n",
        "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    return answer_text\n",
        "\n",
        "# Example 1\n",
        "question1 = \"Which city is Galatasaray based in?\"\n",
        "context1 = \"Galatasaray, is a Turkish professional football club based on the European side of the city of Istanbul. It was founded in 1905.\"\n",
        "\n",
        "teacher_model.eval()\n",
        "student_model.eval()\n",
        "\n",
        "teacher_answer1 = ask_question(question1, context1, teacher_model, tokenizer, device=DEVICE)\n",
        "student_answer1 = ask_question(question1, context1, student_model, tokenizer, device=DEVICE)\n",
        "\n",
        "print(f\"[Teacher Answer]: {teacher_answer1}\")\n",
        "print(f\"[Student Answer]: {student_answer1}\")\n",
        "\n",
        "# Example 2\n",
        "question2 = \"Who wrote the novel 1984?\"\n",
        "context2 = \"George Orwell was a British author best known for his novels Animal Farm and 1984.\"\n",
        "\n",
        "teacher_answer2 = ask_question(question2, context2, teacher_model, tokenizer, device=DEVICE)\n",
        "student_answer2 = ask_question(question2, context2, student_model, tokenizer, device=DEVICE)\n",
        "\n",
        "print(f\"\\nQuestion: {question2}\")\n",
        "print(f\"[Teacher Answer]: {teacher_answer2}\")\n",
        "print(f\"[Student Answer]: {student_answer2}\")"
      ],
      "metadata": {
        "id": "4SpJ6eeO3F21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# 7. FINAL VISUALIZATION: Loss Curves and Performance Comparison\n",
        "# -----------------------------------------------\n",
        "# Plot 1: Training Loss Curve\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epoch_losses, marker='o', linewidth=2)\n",
        "plt.title(\"Training Loss During Fine-Tuning (Post-Pruning)\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Exact Match (EM) and F1 Score Comparison\n",
        "labels = [\"Student Model (Distilled+Pruned)\"]\n",
        "em_scores_plot = [avg_em]\n",
        "f1_scores_plot = [avg_f1]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "rects1 = ax.bar(x - width/2, em_scores_plot, width, label='Exact Match (EM)', color='skyblue')\n",
        "rects2 = ax.bar(x + width/2, f1_scores_plot, width, label='F1 Score', color='lightgreen')\n",
        "\n",
        "def add_labels(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}%',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "add_labels(rects1)\n",
        "add_labels(rects2)\n",
        "\n",
        "ax.set_ylabel('Score (%)')\n",
        "ax.set_title('Validation Performance: EM and F1')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.legend()\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PxgaRa4b4hH2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}