{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaVrabcova/Assessment_2_Mini_Project/blob/main/MSO3255_Assessment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53NDKH8SpLMw"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# MSO3255: Distilled Question Answering Model\n",
        "# Compact BERT with Intermediate Distillation and Pruning\n",
        "# =========================================\n",
        "\n",
        "# -----------------------------------------\n",
        "# 0. SETUP AND DEPENDENCIES\n",
        "# -----------------------------------------\n",
        "\n",
        "# Install Hugging Face datasets library for loading SQuAD\n",
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AZ6J6Zf6pUr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "81ee6b85-fbed-4c25-b7cf-4bb499a592bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b2f379eb0d15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Core libraries for model, training, and data handling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertConfig, BertForQuestionAnswering\n",
        "\n",
        "# Utilities for visualization and token normalization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re, string\n",
        "from torchinfo import summary\n",
        "import os\n",
        "\n",
        "# Enable Google Drive access to store teacher logits\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KHVRE9aLKDfN",
        "outputId": "e87d3db1-f106-47cf-8fae-e7909ca978f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-508d33cd7c98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1. CONFIGURATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mTEACHER_MODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-large-uncased-whole-word-masking-finetuned-squad\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# 1. CONFIGURATION\n",
        "# -----------------------------------------\n",
        "drive.mount('/content/drive')  # Mount Google Drive\n",
        "\n",
        "# Model, training, and distillation parameters\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 3e-5\n",
        "ALPHA = 0.5            # Balance between soft and hard loss\n",
        "TEMPERATURE = 4.0      # Softmax temperature for teacher logits\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pM2lnlMVpfWn",
        "outputId": "decfdaa4-495d-4582-95a0-cfd7afc66243"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b2c5d29cc73b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 2. DATA LOADING & PREPROCESSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mraw_squad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"squad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_squad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_squad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------\n",
        "# 2. DATA LOADING & PREPROCESSING\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load a subset of the SQuAD v1.0 dataset for training and validation\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "train_data = raw_squad[\"train\"].select(range(5000))\n",
        "val_data = raw_squad[\"validation\"].select(range(1000))\n",
        "\n",
        "# Tokenizer initialization\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Preprocessing: tokenize input and align answer positions\n",
        "def preprocess_function(ex):\n",
        "    start_char = ex[\"answers\"][\"answer_start\"][0]\n",
        "    answer_text = ex[\"answers\"][\"text\"][0] if len(ex[\"answers\"][\"text\"]) > 0 else \"\"\n",
        "    encoding = tokenizer(ex[\"question\"], ex[\"context\"], max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_offsets_mapping=True)\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    end_char = start_char + len(answer_text)\n",
        "    start_token_idx = end_token_idx = 0\n",
        "    for i, (off_start, off_end) in enumerate(offsets):\n",
        "        if off_start is None or off_end is None:\n",
        "            continue\n",
        "        if off_start <= start_char < off_end:\n",
        "            start_token_idx = i\n",
        "        if off_start < end_char <= off_end:\n",
        "            end_token_idx = i\n",
        "            break\n",
        "    if end_token_idx < start_token_idx:\n",
        "        end_token_idx = start_token_idx\n",
        "    encoding[\"start_positions\"] = start_token_idx\n",
        "    encoding[\"end_positions\"] = end_token_idx\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "    return encoding\n",
        "\n",
        "train_processed = train_data.map(preprocess_function)\n",
        "val_processed = val_data.map(preprocess_function)\n",
        "\n",
        "# Convert Hugging Face dataset to PyTorch tensors\n",
        "def to_tensor_dataset(hf_dataset):\n",
        "    input_ids = torch.tensor(hf_dataset[\"input_ids\"], dtype=torch.long)\n",
        "    attention_mask = torch.tensor(hf_dataset[\"attention_mask\"], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor(hf_dataset[\"token_type_ids\"], dtype=torch.long) if \"token_type_ids\" in hf_dataset.features else None\n",
        "    start_positions = torch.tensor(hf_dataset[\"start_positions\"], dtype=torch.long)\n",
        "    end_positions = torch.tensor(hf_dataset[\"end_positions\"], dtype=torch.long)\n",
        "    if token_type_ids is not None:\n",
        "        return TensorDataset(input_ids, attention_mask, token_type_ids, start_positions, end_positions)\n",
        "    return TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "\n",
        "train_tds = to_tensor_dataset(train_processed)\n",
        "val_tds = to_tensor_dataset(val_processed)\n",
        "train_loader = DataLoader(train_tds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_tds, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4NcBFhsTp541"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------\n",
        "# 3. TEACHER MODEL INFERENCE\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load and run the teacher model to generate soft labels (logits)\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE).eval()\n",
        "\n",
        "teacher_start_logits_list, teacher_end_logits_list = [], []\n",
        "gt_start_list, gt_end_list, input_tensors = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "        input_ids, attention_mask, start_pos, end_pos = input_ids.to(DEVICE), attention_mask.to(DEVICE), start_pos.to(DEVICE), end_pos.to(DEVICE)\n",
        "        outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "        teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "        gt_start_list.append(start_pos.cpu())\n",
        "        gt_end_list.append(end_pos.cpu())\n",
        "        input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu() if token_type_ids is not None else None))\n",
        "\n",
        "# Aggregate all batches into single tensors\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten the inputs for use in distillation\n",
        "all_input_ids, all_attention_masks, all_token_type_ids = [], [], []\n",
        "for i_ids, i_mask, i_type in input_tensors:\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "    else:\n",
        "        all_token_type_ids = None\n",
        "\n",
        "all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "\n",
        "# Save teacher logits and metadata to disk (Google Drive)\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/teacher_logits.pt\"\n",
        "torch.save({\n",
        "    \"start_logits\": teacher_start_logits_full,\n",
        "    \"end_logits\": teacher_end_logits_full,\n",
        "    \"gt_start\": gt_start_full,\n",
        "    \"gt_end\": gt_end_full,\n",
        "    \"input_ids\": all_input_ids,\n",
        "    \"attention_mask\": all_attention_masks,\n",
        "    \"token_type_ids\": all_token_type_ids\n",
        "}, LOGITS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# 4. BUILD STUDENT MODEL\n",
        "# -----------------------------------------\n",
        "\n",
        "# Define a smaller BERT model to serve as the student\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,\n",
        "    num_hidden_layers=8,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)\n",
        "\n",
        "# Show summary of the student model architecture\n",
        "summary(student_model, input_size=(BATCH_SIZE, MAX_LENGTH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Me8vTvyNPI3u",
        "outputId": "fd1e18af-716a-47aa-e453-3b28657bda06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BertConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-353c7310e7eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 4. BUILD STUDENT MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m student_config = BertConfig(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# 5. DISTILLATION TRAINING + PRUNING\n",
        "# -----------------------------------------\n",
        "\n",
        "# Define projection layer to match hidden states\n",
        "project_teacher = nn.Linear(1024, 384).to(DEVICE)\n",
        "\n",
        "# Define loss functions\n",
        "kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")  # KL Divergence for soft targets\n",
        "ce_loss_fn = nn.CrossEntropyLoss()                  # Cross-entropy for hard targets\n",
        "mse_loss_fn = nn.MSELoss()                           # MSE for hidden state matching\n",
        "\n",
        "# Apply pruning (removes 10% of least important weights)\n",
        "def apply_pruning(model, amount=0.1):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.utils.prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            torch.nn.utils.prune.remove(module, \"weight\")\n",
        "\n",
        "apply_pruning(student_model, amount=0.1)\n",
        "print(\"Remaining trainable parameters:\", sum(p.numel() for p in student_model.parameters() if p.requires_grad))\n",
        "\n",
        "# Define optimizer after pruning\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Prepare distillation dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs = softmax_with_temperature(teacher_end_logits_full, TEMPERATURE)\n",
        "\n",
        "# Rebuild distillation dataset (handling optional token_type_ids)\n",
        "if all_token_type_ids is not None:\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids, all_attention_masks, all_token_type_ids,\n",
        "        teacher_start_probs, teacher_end_probs,\n",
        "        gt_start_full, gt_end_full\n",
        "    )\n",
        "else:\n",
        "    distill_dataset = TensorDataset(\n",
        "        all_input_ids, all_attention_masks, torch.zeros_like(all_input_ids),\n",
        "        teacher_start_probs, teacher_end_probs,\n",
        "        gt_start_full, gt_end_full\n",
        "    )\n",
        "\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "Y0NkGWx0PJBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# DISTILLATION TRAINING LOOP\n",
        "# =====================================\n",
        "\n",
        "student_model.train()\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n[Epoch {epoch+1}/{EPOCHS}] Starting distillation training...\")\n",
        "    running_loss = []\n",
        "    for step, batch in enumerate(distill_loader):\n",
        "        # Unpack batch\n",
        "        if all_token_type_ids is not None:\n",
        "            input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, _, t_start_probs, t_end_probs, gt_start, gt_end = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        # Move tensors to device\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "        t_start_probs = t_start_probs.to(DEVICE)\n",
        "        t_end_probs = t_end_probs.to(DEVICE)\n",
        "        gt_start = gt_start.to(DEVICE)\n",
        "        gt_end = gt_end.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass teacher (hidden states)\n",
        "        with torch.no_grad():\n",
        "            teacher_out = teacher_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
        "            teacher_hidden = teacher_out.hidden_states[TEACHER_LAYER_TO_MATCH]\n",
        "            teacher_proj = project_teacher(teacher_hidden)\n",
        "\n",
        "        # Forward pass student\n",
        "        student_out = student_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
        "        student_hidden = student_out.hidden_states[STUDENT_LAYER_TO_MATCH]\n",
        "\n",
        "        # Compute student outputs\n",
        "        s_start_logits = student_out.start_logits\n",
        "        s_end_logits = student_out.end_logits\n",
        "        s_start_probs = softmax_with_temperature(s_start_logits, TEMPERATURE)\n",
        "        s_end_probs = softmax_with_temperature(s_end_logits, TEMPERATURE)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_hard = 0.5 * (ce_loss_fn(s_start_logits, gt_start) + ce_loss_fn(s_end_logits, gt_end))\n",
        "        loss_soft = 0.5 * (kl_loss_fn(s_start_probs.log(), t_start_probs) + kl_loss_fn(s_end_probs.log(), t_end_probs))\n",
        "        loss_hidden = mse_loss_fn(student_hidden, teacher_proj)\n",
        "\n",
        "        # Total distillation loss\n",
        "        loss = ALPHA * loss_soft + (1 - ALPHA) * loss_hard + HIDDEN_LOSS_WEIGHT * loss_hidden\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    epoch_avg = np.mean(running_loss)\n",
        "    epoch_losses.append(epoch_avg)\n",
        "    print(f\"[Epoch {epoch+1}] Average Loss: {epoch_avg:.4f}\")"
      ],
      "metadata": {
        "id": "aDwXXALSPJEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# 6. PLOT TRAINING LOSS\n",
        "# -----------------------------------------\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(epoch_losses, marker='o')\n",
        "plt.title(\"Distillation Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BnRZjFybPJHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# 7. FINAL VALIDATION EVALUATION (EM + F1)\n",
        "# -----------------------------------------\n",
        "\n",
        "student_model.eval()\n",
        "em_scores = []\n",
        "f1_scores = []\n",
        "offset = 0\n",
        "\n",
        "for batch in val_loader:\n",
        "    if len(batch) == 5:\n",
        "        input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "    else:\n",
        "        input_ids, attention_mask, start_pos, end_pos = batch\n",
        "        token_type_ids = None\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "    end_indices = torch.argmax(end_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    for i in range(len(start_indices)):\n",
        "        s_idx = start_indices[i]\n",
        "        e_idx = end_indices[i]\n",
        "        if e_idx < s_idx:\n",
        "            e_idx = s_idx\n",
        "        pred_tokens = input_ids[i][s_idx:e_idx+1]\n",
        "        pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
        "\n",
        "        gold_text = val_data[offset+i]['answers']['text'][0] if len(val_data[offset+i]['answers']['text']) > 0 else \"\"\n",
        "\n",
        "        em = int(normalize_text(pred_text) == normalize_text(gold_text))\n",
        "        f1 = compute_f1(pred_text, gold_text)\n",
        "\n",
        "        em_scores.append(em)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    offset += len(start_indices)\n",
        "\n",
        "# Report evaluation metrics\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"Exact Match (EM): {np.mean(em_scores)*100:.2f}%\")\n",
        "print(f\"F1 Score: {np.mean(f1_scores)*100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f6WGdixaPJKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------\n",
        "# BAR PLOT: EM and F1 Comparison\n",
        "# -----------------------------------------\n",
        "\n",
        "labels = [\"Baseline\", \"Distilled+Tuned\", \"Pruned+Fine-tuned\"]\n",
        "em_vals = [2.8, 2.0, 1.3]\n",
        "f1_vals = [6.9, 6.59, 5.3]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rect1 = ax.bar(x - width/2, em_vals, width, label=\"EM\", color=\"cornflowerblue\")\n",
        "rect2 = ax.bar(x + width/2, f1_vals, width, label=\"F1\", color=\"mediumseagreen\")\n",
        "\n",
        "ax.set_ylabel(\"Score (%)\")\n",
        "ax.set_title(\"QA Performance by Model Variant\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "for r in rect1 + rect2:\n",
        "    height = r.get_height()\n",
        "    ax.annotate(f'{height:.2f}',\n",
        "                xy=(r.get_x() + r.get_width() / 2, height),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "imPCNRSxPJNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}